![image](https://github.com/rasieira98/EcoForecast-23/assets/116558787/61327ed2-ab7b-458d-9f6e-fbd43d0597a8)
- INTRODUCCION
- DATA INGESTION
- DATA PREPROCESSING
- TRAINING
- EXAMPLE LOG OF AN EXECUTION
- RESULTS IN TEST -> 'f1_macro': 0.7751518086158283
***
# INTRODUCCION
This GitHub repository contains a project aimed at calculating the energy surplus for each country in the next hour, addressing Schneider's challenge on the NUWE platform.

If you want to execute all pipelines with only one command:
```
./scripts/run_pipeline.sh 2022-01-01 2023-01-01 data data/raw_data.csv data/processed_data.csv models/model.pkl data/test_data.csv predictions/predictions.json
```
***
# DATA INGESTION
### Description
It collects data from the ENTSO-E Transparency API to get how the electricity consumption (Load) and the generation of different types of energy. 

In this script there are several steps and considerations which were kept in mind during the ingestion:

1. Download the data from 01-01-2022 to 01-01-2023
2. If it was necessary to download more data, there's a method to split the calls year by year (Max period to query using the API = 1 year).
3. In order to speed up the collection, we use threads (1 thread by country). The result is to collect all data in less than 5 minutes.
4. After the data extraction, we have a lot of files by country (Load and gen) splitted by every type of energy (Renewables and not green energy). Then, in order to gather all this data in only one file, a preprocessing was implemented. We build the raw_data.csv by using a pivot function. 
### Output
The output of this process has the following format:
```
Country,StartTime,EndTime,AreaID,UnitName,B01,B02,B03,B04,B05,B06,B07,B08,B09,B10,B11,B12,B13,B14,B15,B16,B17,B18,B19,B20,Load
DE,2022-01-01T00:00+00:00Z,2022-01-01T00:15+00:00Z,10Y1001A1001A83F,MAW,4325.0,3521.0,,2628.0,2057.0,279.0,,,26.0,568.0,1421.0,121.0,,3350.0,126.0,0.0,841.0,5795.0,24604.0,333.0,41804.0
```

### Logging
The script uses a custom logger named "Data_ingestion" to log its progress. Logs are informative and include details about data ingestion.
### Example Usage
```
python data_ingestion.py --start_time 2022-01-01 --end_time 2023-01-01
```

***
# DATA PREPROCESSING
***
### Description
In this script, we preprocess the data generated in the previous step of the pipeline. The steps which were considered for this process are described below:
1. Convert the 'StartTime' column to datetime format.
2. Sort the DataFrame based on the 'StartTime' column.
3. Filter out rows where 'AreaID' is null, because the API returned records with AreaID = null
4. We filtered the renewable columns, which are configured in the config.yaml file (Variable columns_clean)
5. Remove outliers from the input DataFrame based on Z-scores
6. Interpolate missing values by interval for each country. For example, if at 15:00 is null, 15:15 is null and 15:45 is null, but 15:30=100, then we interpolate this interval (Result is 15:00, 15:15 and 15:45 = 100). If it's empty, the result for this interval is None. During this process, we create a dummy dataframe with all intervals in two specifics dates (2022-01-01 and 2022-12-31 for this case) in order to have 8760 records by country (UK with a lot of nulls values).
7. Resample the DataFrame to hourly frequency. If all values are NaN, then the result is NaN, not 0.
8. Create columns 'green_energy' and 'surplus' based on specified calculations. If green_enery or Load is NaN, then the surplus is NaN too for that country.
9. Create a target table with columns 'StartTime' and 'target'.
10. Pivot the final table based on specified columns.
11. Merge the pivot table with the target table.
12. Finally, we decided not to filter the rows by using the dropna function, because we believe that in a real case, we would have to consider the bigger quantity of data. Therefore, 8760 records were considered for the next step.

### Output
The output of this process has the following format:
```
StartTime,B01_DE,B01_DK,B01_HU,B01_IT,B01_NE,B01_PO,B01_SE,B01_SP,B01_UK,B09_DE,B09_DK,B09_HU,B09_IT,B09_NE,B09_PO,B09_SE,B09_SP,B09_UK,B10_DE,B10_DK,B10_HU,B10_IT,B10_NE,B10_PO,B10_SE,B10_SP,B10_UK,B11_DE,B11_DK,B11_HU,B11_IT,B11_NE,B11_PO,B11_SE,B11_SP,B11_UK,B12_DE,B12_DK,B12_HU,B12_IT,B12_NE,B12_PO,B12_SE,B12_SP,B12_UK,B13_DE,B13_DK,B13_HU,B13_IT,B13_NE,B13_PO,B13_SE,B13_SP,B13_UK,B15_DE,B15_DK,B15_HU,B15_IT,B15_NE,B15_PO,B15_SE,B15_SP,B15_UK,B16_DE,B16_DK,B16_HU,B16_IT,B16_NE,B16_PO,B16_SE,B16_SP,B16_UK,B18_DE,B18_DK,B18_HU,B18_IT,B18_NE,B18_PO,B18_SE,B18_SP,B18_UK,B19_DE,B19_DK,B19_HU,B19_IT,B19_NE,B19_PO,B19_SE,B19_SP,B19_UK,Load_DE,Load_DK,Load_HU,Load_IT,Load_NE,Load_PO,Load_SE,Load_SP,Load_UK,green_energy_DE,green_energy_DK,green_energy_HU,green_energy_IT,green_energy_NE,green_energy_PO,green_energy_SE,green_energy_SP,green_energy_UK,surplus_DE,surplus_DK,surplus_HU,surplus_IT,surplus_NE,surplus_PO,surplus_SE,surplus_SP,surplus_UK,target
2022-01-01 00:00:00,17295.0,511.0,523.0,683.0,85.0,221.0,,524.0,,104.0,,0.0,640.0,,,,0.0,,2274.0,,,0.0,,0.0,,,,5690.0,,44.0,1951.0,0.0,199.0,,1046.0,,378.0,,36.0,200.0,,0.0,7086.0,746.0,,,,,,,,0.0,0.0,,504.0,,30.0,,,,,96.0,,0.0,1.0,0.0,0.0,0.0,0.0,0.0,75.0,,23112.0,1904.0,,,6884.0,,,0.0,,95800.0,1189.0,743.0,2140.0,5739.0,4071.0,4021.0,6456.0,,165125.0,3218.0,16457.0,19756.0,40706.0,13935.0,15331.0,19530.0,1244.0,145157.0,3605.0,1376.0,5614.0,12708.0,4491.0,11107.0,8943.0,,-19968.0,387.0,-15081.0,-14142.0,-27998.0,-9444.0,-4224.0,-10587.0,,DK
2022-01-01 01:00:00,17322.0,519.0,516.0,670.0,85.0,223.0,,532.0,,106.0,,0.0,639.0,,,,0.0,,1392.0,,,0.0,,0.0,,,,5683.0,,44.0,1831.0,0.0,216.0,,1045.0,,132.0,,36.0,155.0,,0.0,7088.0,733.0,,,,,,,,0.0,0.0,,504.0,,28.0,,,,,96.0,,0.0,1.0,0.0,0.0,0.0,0.0,0.0,75.0,,21798.0,1738.0,,,5518.0,,,0.0,,91464.0,1051.0,902.0,2233.0,5494.0,3997.0,3948.0,6144.0,,160415.0,3126.0,15426.0,18685.0,39465.0,13579.0,15270.0,18383.0,1131.0,138401.0,3309.0,1526.0,5528.0,11097.0,4436.0,11036.0,8625.0,,-22014.0,183.0,-13900.0,-13157.0,-28368.0,-9143.0,-4234.0,-9758.0,,DK
```

### Configuration
Ensure that the config.py file is properly configured with relevant settings for data processing.


### Logging
The script uses a custom logger named "Data_processing" to log its progress. Logs are informative and include details about data processing, such as different metrics in each stage.
### Example Usage
```
python data_processing.py --input_file data/raw_data.csv --output_file data/processed_data.csv
```

# TRAINING
### Overview
Is designed to streamline the process of train a model on a given dataset. The functionality is structured into modular functions that handle loading data, splitting data, train and save the model.
### Usage
1. Install Dependencies
Ensure that the required dependencies are installed. You can install them using the following command:
```
pip install pandas autogluon.tabular sklearn
```
2. Prepare Configuration
Make sure that the config.py file is properly configured. This file likely contains settings related to training process.

3. Run the Script
Execute the script from the command line with the following arguments:

```
python model_training.py --input_file path/to/processed_data.csv --model_file path/to/trained_model.pkl
```
- --input_file: Path to the CSV file containing the procesed data.
- --model_file: Path to save the pre-trained model file.
- 
The script will log its progress, and the final model will be saved in the specified output file.

### Script Structure
- load_data(file_path: str) -> pd.DataFrame:
Description: Load data from a CSV file and return it as a Pandas DataFrame.
- split_data(df: pd.DataFrame) -> pd.DataFrame:
Description: Split the input DataFrame into training and testing sets, save the sets as CSV files, and return the training set.
- train_and_save_model(df_train: pd.DataFrame, model_path: str):
Description: Train a model using the TabularPredictor from the `autogluon.tabular` module, and save the trained model to the specified path.
- parse_arguments():
Description: Parse command-line arguments for the model training script.
- main(input_file, model_file):
Description: Main function that orchestrates the entire train process. It loads data, train the model and save the model.

### Dependencies
- argparse: Used for parsing command-line arguments.
- pandas: Essential for working with tabular data.
- autogluon: Provides the TabularPredictor class for working with tabular data.
### Configuration
Ensure that the config.py file is properly configured with relevant settings for data processing.

### Logging
The script uses a custom logger named "Model_training" to log its progress. Logs are informative and include details about data loading, model training and model save.
### Example Usage
```
python model_training.py --input_file path/to/processed_data.csv --model_file path/to/trained_model.pkl
```
***
# PREDICT
### Overview
Is designed to streamline the process of making predictions using a pre-trained model on a given dataset. The functionality is structured into modular functions that handle loading data, loading a pre-trained model, making predictions, and saving the results in a desired format.
### Usage
1. Install Dependencies
Ensure that the required dependencies are installed. You can install them using the following command:
```
pip install pandas autogluon.tabular
```
2. Prepare Configuration
Make sure that the config.py file is properly configured. This file likely contains settings related to data processing and mapping.

3. Run the Script
Execute the script from the command line with the following arguments:

```
python model_prediction.py --input_file path/to/test_data.csv --model_file path/to/trained_model.pkl --output_file path/to/predictions.json
```
- --input_file: Path to the CSV file containing the test data.
- --model_file: Path to the pre-trained model file.
- --output_file: Path to save the predictions in JSON format.
The script will log its progress, and the final predictions will be saved in the specified output file.

### Script Structure
- load_data(file_path: str) -> pd.DataFrame
Description: Loads test data from a CSV file and returns it as a Pandas DataFrame.
- load_model(model_path: str) -> TabularPredictor
Description: Loads a pre-trained model from the specified path using the TabularPredictor.
- make_predictions(df: pd.DataFrame, model: TabularPredictor) -> dict
Description: Makes predictions using a trained model on the provided DataFrame and returns the predictions in a dictionary format.
- save_predictions(predictions, predictions_file: str)
Description: Saves predictions to a JSON file.
- parse_arguments() -> argparse.Namespace
Description: Parses command-line arguments for the model prediction script and returns an object containing the parsed arguments.
- main(input_file, model_file, output_file)
Description: Main function that orchestrates the entire prediction process. It loads data, loads the model, makes predictions, and saves the results.

Description: Parses command-line arguments and executes the main function when the script is run.
### Dependencies
- argparse: Used for parsing command-line arguments.
- json: Utilized for reading and writing JSON files.
- pandas: Essential for working with tabular data.
- autogluon: Provides the TabularPredictor class for working with tabular data and making predictions.
### Configuration
Ensure that the config.py file is properly configured with relevant settings for data processing.

### Logging
The script uses a custom logger named "Model_prediction" to log its progress. Logs are informative and include details about data loading, model loading, evaluation, and predictions.
### Example Usage
```
python model_prediction.py --input_file data/test_data.csv --model_file models/model.pkl --output_file predictions/predictions.json
```
***
For further details on each function and their parameters, refer to the function docstrings in the code.
***
# IMPORTANT: EXAMPLE LOG OF AN EXECUTION:
```
Starting data ingestion...
2023-11-20 19:59:04,545 INFO     Starting...
2023-11-20 19:59:04,545 INFO     Starting the process to get the load data...
2023-11-20 19:59:04,545 INFO     HU Fetching data...
2023-11-20 19:59:04,553 INFO     IT Fetching data...
2023-11-20 19:59:04,553 INFO     PO Fetching data...
2023-11-20 19:59:04,553 INFO     SP Fetching data...
2023-11-20 19:59:04,553 INFO     UK Fetching data...
2023-11-20 19:59:04,553 INFO     DE Fetching data...
2023-11-20 19:59:04,553 INFO     DK Fetching data...
2023-11-20 19:59:04,561 INFO     SE Fetching data...
2023-11-20 19:59:04,562 INFO     NE Fetching data...
2023-11-20 19:59:09,604 INFO     [PO] Writing 8760 points in data/load_PO.csv...
2023-11-20 19:59:13,749 INFO     [HU] Writing 35040 points in data/load_HU.csv...
2023-11-20 19:59:14,090 INFO     [HU] Load Data process has finished successfully.
2023-11-20 19:59:14,327 INFO     [IT] Writing 8760 points in data/load_IT.csv...
2023-11-20 19:59:14,400 INFO     [IT] Load Data process has finished successfully.
2023-11-20 19:59:14,400 INFO     [PO] Load Data process has finished successfully.
2023-11-20 19:59:16,884 INFO     [SE] Writing 8760 points in data/load_SE.csv...
2023-11-20 19:59:17,338 INFO     [SP] Writing 24816 points in data/load_SP.csv...
2023-11-20 19:59:17,539 INFO     [SP] Load Data process has finished successfully.
2023-11-20 19:59:19,889 INFO     [NE] Writing 35040 points in data/load_NE.csv...
2023-11-20 19:59:25,820 INFO     [UK] Writing 9446 points in data/load_UK.csv...
2023-11-20 19:59:25,884 INFO     [UK] Load Data process has finished successfully.
2023-11-20 19:59:27,945 INFO     [DK] Writing 8760 points in data/load_DK.csv...
2023-11-20 19:59:34,904 INFO     [DE] Writing 35040 points in data/load_DE.csv...
2023-11-20 19:59:35,198 INFO     [DE] Load Data process has finished successfully.
2023-11-20 19:59:35,198 INFO     [DK] Load Data process has finished successfully.
2023-11-20 19:59:35,198 INFO     [SE] Load Data process has finished successfully.
2023-11-20 19:59:35,198 INFO     [NE] Load Data process has finished successfully.
2023-11-20 19:59:35,198 INFO     <<<<<<<<<< Load Data got successfully for all countries. >>>>>>>>>>
2023-11-20 19:59:35,198 INFO     Duration to get the load data from entsoe: 30.64 seconds
2023-11-20 19:59:35,198 INFO     Starting the generation data's extraction...
2023-11-20 19:59:35,198 INFO     [HU] Fetching data...
2023-11-20 19:59:35,207 INFO     [HU] Collecting data from 202201010000 to 202301010000
2023-11-20 19:59:35,207 INFO     [IT] Fetching data...
2023-11-20 19:59:35,207 INFO     [PO] Fetching data...
2023-11-20 19:59:35,207 INFO     [IT] Collecting data from 202201010000 to 202301010000
2023-11-20 19:59:35,207 INFO     [SP] Fetching data...
2023-11-20 19:59:35,207 INFO     [PO] Collecting data from 202201010000 to 202301010000
2023-11-20 19:59:35,216 INFO     [UK] Fetching data...
2023-11-20 19:59:35,216 INFO     [DE] Fetching data...
2023-11-20 19:59:35,216 INFO     [SP] Collecting data from 202201010000 to 202301010000
2023-11-20 19:59:35,223 INFO     [DK] Fetching data...
2023-11-20 19:59:35,223 INFO     [SE] Fetching data...
2023-11-20 19:59:35,223 INFO     [UK] Collecting data from 202201010000 to 202301010000
2023-11-20 19:59:35,223 INFO     [NE] Fetching data...
2023-11-20 19:59:35,223 INFO     [DE] Collecting data from 202201010000 to 202301010000
2023-11-20 19:59:35,234 INFO     [DK] Collecting data from 202201010000 to 202301010000
2023-11-20 19:59:35,234 INFO     [SE] Collecting data from 202201010000 to 202301010000
2023-11-20 19:59:35,239 INFO     [NE] Collecting data from 202201010000 to 202301010000
2023-11-20 20:00:11,150 INFO     [PO] - Writing 8760 points in data/gen_PO_B01.csv...
2023-11-20 20:00:11,192 INFO     [PO] - Writing 8760 points in data/gen_PO_B02.csv...
2023-11-20 20:00:11,224 INFO     [PO] - Writing 8760 points in data/gen_PO_B03.csv...
2023-11-20 20:00:11,257 INFO     [PO] - Writing 8760 points in data/gen_PO_B04.csv...
2023-11-20 20:00:11,281 INFO     [PO] - Writing 8760 points in data/gen_PO_B05.csv...
2023-11-20 20:00:11,329 INFO     [PO] - Writing 8760 points in data/gen_PO_B06.csv...
2023-11-20 20:00:11,354 INFO     [PO] - Writing 8760 points in data/gen_PO_B10.csv...
2023-11-20 20:00:11,394 INFO     [PO] - Writing 8760 points in data/gen_PO_B11.csv...
2023-11-20 20:00:11,411 INFO     [PO] - Writing 8760 points in data/gen_PO_B12.csv...
2023-11-20 20:00:11,435 INFO     [PO] - Writing 8760 points in data/gen_PO_B16.csv...
2023-11-20 20:00:11,459 INFO     [PO] - Writing 8760 points in data/gen_PO_B19.csv...
2023-11-20 20:00:24,697 INFO     [SE] - Writing 8759 points in data/gen_SE_B04.csv...
2023-11-20 20:00:24,713 INFO     [SE] - Writing 8759 points in data/gen_SE_B12.csv...
2023-11-20 20:00:24,737 INFO     [SE] - Writing 1031 points in data/gen_SE_B13.csv...
2023-11-20 20:00:24,745 INFO     [SE] - Writing 8759 points in data/gen_SE_B14.csv...
2023-11-20 20:00:24,769 INFO     [SE] - Writing 8759 points in data/gen_SE_B20.csv...
2023-11-20 20:00:24,786 INFO     [SE] - Writing 8759 points in data/gen_SE_B16.csv...
2023-11-20 20:00:24,818 INFO     [SE] - Writing 8759 points in data/gen_SE_B19.csv...
2023-11-20 20:00:51,696 INFO     [IT] - Writing 8760 points in data/gen_IT_B01.csv...
2023-11-20 20:00:51,722 INFO     [IT] - Writing 8760 points in data/gen_IT_B03.csv...
2023-11-20 20:00:51,753 INFO     [IT] - Writing 8760 points in data/gen_IT_B04.csv...
2023-11-20 20:00:51,777 INFO     [IT] - Writing 8760 points in data/gen_IT_B05.csv...
2023-11-20 20:00:51,801 INFO     [IT] - Writing 8760 points in data/gen_IT_B06.csv...
2023-11-20 20:00:51,829 INFO     [IT] - Writing 8760 points in data/gen_IT_B09.csv...
2023-11-20 20:00:51,850 INFO     [IT] - Writing 8760 points in data/gen_IT_B10.csv...
2023-11-20 20:00:51,900 INFO     [IT] - Writing 8760 points in data/gen_IT_B11.csv...
2023-11-20 20:00:51,929 INFO     [IT] - Writing 8760 points in data/gen_IT_B12.csv...
2023-11-20 20:00:51,956 INFO     [IT] - Writing 8760 points in data/gen_IT_B20.csv...
2023-11-20 20:00:51,981 INFO     [IT] - Writing 8760 points in data/gen_IT_B16.csv...
2023-11-20 20:00:52,005 INFO     [IT] - Writing 8760 points in data/gen_IT_B17.csv...
2023-11-20 20:00:52,029 INFO     [IT] - Writing 5258 points in data/gen_IT_B18.csv...
2023-11-20 20:00:52,046 INFO     [IT] - Writing 8760 points in data/gen_IT_B19.csv...
2023-11-20 20:01:13,570 INFO     [DK] - Writing 8758 points in data/gen_DK_B01.csv...
2023-11-20 20:01:13,595 INFO     [DK] - Writing 8757 points in data/gen_DK_B04.csv...
2023-11-20 20:01:13,619 INFO     [DK] - Writing 8758 points in data/gen_DK_B05.csv...
2023-11-20 20:01:13,645 INFO     [DK] - Writing 8758 points in data/gen_DK_B06.csv...
2023-11-20 20:01:13,669 INFO     [DK] - Writing 8760 points in data/gen_DK_B16.csv...
2023-11-20 20:01:13,702 INFO     [DK] - Writing 8758 points in data/gen_DK_B17.csv...
2023-11-20 20:01:13,726 INFO     [DK] - Writing 8760 points in data/gen_DK_B18.csv...
2023-11-20 20:01:13,751 INFO     [DK] - Writing 8760 points in data/gen_DK_B19.csv...
2023-11-20 20:01:16,044 INFO     [UK] - Writing 4334 points in data/gen_UK_B04.csv...
2023-11-20 20:01:16,069 INFO     [UK] - Writing 4334 points in data/gen_UK_B05.csv...
2023-11-20 20:01:16,093 INFO     [UK] - Writing 4334 points in data/gen_UK_B06.csv...
2023-11-20 20:01:16,118 INFO     [UK] - Writing 4330 points in data/gen_UK_B20.csv...
2023-11-20 20:01:16,143 INFO     [UK] - Writing 6322 points in data/gen_UK_B19.csv...
2023-11-20 20:02:11,484 INFO     [SP] - Writing 24816 points in data/gen_SP_B01.csv...
2023-11-20 20:02:11,541 INFO     [SP] - Writing 24816 points in data/gen_SP_B02.csv...
2023-11-20 20:02:11,606 INFO     [SP] - Writing 24816 points in data/gen_SP_B03.csv...
2023-11-20 20:02:11,663 INFO     [SP] - Writing 24816 points in data/gen_SP_B04.csv...
2023-11-20 20:02:11,737 INFO     [SP] - Writing 24816 points in data/gen_SP_B05.csv...
2023-11-20 20:02:11,803 INFO     [SP] - Writing 24816 points in data/gen_SP_B06.csv...
2023-11-20 20:02:11,859 INFO     [SP] - Writing 24816 points in data/gen_SP_B07.csv...
2023-11-20 20:02:11,925 INFO     [SP] - Writing 24816 points in data/gen_SP_B08.csv...
2023-11-20 20:02:11,990 INFO     [SP] - Writing 24816 points in data/gen_SP_B09.csv...
2023-11-20 20:02:12,047 INFO     [SP] - Writing 24816 points in data/gen_SP_B10.csv...
2023-11-20 20:02:12,121 INFO     [SP] - Writing 24816 points in data/gen_SP_B11.csv...
2023-11-20 20:02:12,186 INFO     [SP] - Writing 24816 points in data/gen_SP_B12.csv...
2023-11-20 20:02:12,251 INFO     [SP] - Writing 24816 points in data/gen_SP_B13.csv...
2023-11-20 20:02:12,315 INFO     [SP] - Writing 24816 points in data/gen_SP_B14.csv...
2023-11-20 20:02:12,380 INFO     [SP] - Writing 24816 points in data/gen_SP_B20.csv...
2023-11-20 20:02:12,463 INFO     [SP] - Writing 24816 points in data/gen_SP_B15.csv...
2023-11-20 20:02:12,535 INFO     [SP] - Writing 24816 points in data/gen_SP_B16.csv...
2023-11-20 20:02:12,609 INFO     [SP] - Writing 24816 points in data/gen_SP_B17.csv...
2023-11-20 20:02:12,675 INFO     [SP] - Writing 24816 points in data/gen_SP_B18.csv...
2023-11-20 20:02:12,733 INFO     [SP] - Writing 24816 points in data/gen_SP_B19.csv...
2023-11-20 20:03:04,744 INFO     [DE] - Writing 35040 points in data/gen_DE_B01.csv...
2023-11-20 20:03:04,932 INFO     [DE] - Writing 35040 points in data/gen_DE_B02.csv...
2023-11-20 20:03:05,112 INFO     [DE] - Writing 35040 points in data/gen_DE_B04.csv...
2023-11-20 20:03:05,316 INFO     [DE] - Writing 35040 points in data/gen_DE_B05.csv...
2023-11-20 20:03:05,502 INFO     [DE] - Writing 35040 points in data/gen_DE_B06.csv...
2023-11-20 20:03:05,730 INFO     [DE] - Writing 35039 points in data/gen_DE_B09.csv...
2023-11-20 20:03:05,910 INFO     [DE] - Writing 35040 points in data/gen_DE_B10.csv...
2023-11-20 20:03:06,287 INFO     [DE] - Writing 35040 points in data/gen_DE_B11.csv...
2023-11-20 20:03:06,531 INFO     [DE] - Writing 35040 points in data/gen_DE_B12.csv...
2023-11-20 20:03:06,783 INFO     [DE] - Writing 35040 points in data/gen_DE_B14.csv...
2023-11-20 20:03:07,016 INFO     [DE] - Writing 35040 points in data/gen_DE_B20.csv...
2023-11-20 20:03:07,252 INFO     [DE] - Writing 35040 points in data/gen_DE_B15.csv...
2023-11-20 20:03:07,463 INFO     [DE] - Writing 35040 points in data/gen_DE_B16.csv...
2023-11-20 20:03:07,766 INFO     [DE] - Writing 35040 points in data/gen_DE_B17.csv...
2023-11-20 20:03:07,961 INFO     [DE] - Writing 35040 points in data/gen_DE_B18.csv...
2023-11-20 20:03:08,132 INFO     [DE] - Writing 35040 points in data/gen_DE_B19.csv...
2023-11-20 20:03:24,094 INFO     [NE] - Writing 35040 points in data/gen_NE_B01.csv...
2023-11-20 20:03:24,419 INFO     [NE] - Writing 35040 points in data/gen_NE_B04.csv...
2023-11-20 20:03:24,751 INFO     [NE] - Writing 35040 points in data/gen_NE_B05.csv...
2023-11-20 20:03:30,872 INFO     [NE] - Writing 35040 points in data/gen_NE_B11.csv...
2023-11-20 20:03:33,500 INFO     [NE] - Writing 35040 points in data/gen_NE_B14.csv...
2023-11-20 20:03:35,607 INFO     [NE] - Writing 35040 points in data/gen_NE_B20.csv...
2023-11-20 20:03:37,228 INFO     [NE] - Writing 35040 points in data/gen_NE_B16.csv...
2023-11-20 20:03:37,829 INFO     [HU] - Writing 35040 points in data/gen_HU_B01.csv...
2023-11-20 20:03:38,129 INFO     [HU] - Writing 35040 points in data/gen_HU_B02.csv...
2023-11-20 20:03:38,204 INFO     [NE] - Writing 35040 points in data/gen_NE_B17.csv...
2023-11-20 20:03:38,423 INFO     [HU] - Writing 35040 points in data/gen_HU_B04.csv...
2023-11-20 20:03:38,780 INFO     [HU] - Writing 35040 points in data/gen_HU_B05.csv...
2023-11-20 20:03:38,873 INFO     [NE] - Writing 35040 points in data/gen_NE_B18.csv...
2023-11-20 20:03:39,075 INFO     [HU] - Writing 35040 points in data/gen_HU_B06.csv...
2023-11-20 20:03:39,441 INFO     [NE] - Writing 35040 points in data/gen_NE_B19.csv...
2023-11-20 20:03:39,473 INFO     [HU] - Writing 35040 points in data/gen_HU_B09.csv...
2023-11-20 20:03:39,831 INFO     [HU] - Writing 35040 points in data/gen_HU_B11.csv...
2023-11-20 20:03:40,117 INFO     [HU] - Writing 35040 points in data/gen_HU_B12.csv...
2023-11-20 20:03:40,320 INFO     [HU] - Writing 35040 points in data/gen_HU_B14.csv...
2023-11-20 20:03:40,479 INFO     [HU] - Writing 35040 points in data/gen_HU_B20.csv...
2023-11-20 20:03:40,636 INFO     [HU] - Writing 35040 points in data/gen_HU_B15.csv...
2023-11-20 20:03:40,798 INFO     [HU] - Writing 35040 points in data/gen_HU_B16.csv...
2023-11-20 20:03:40,947 INFO     [HU] - Writing 35040 points in data/gen_HU_B17.csv...
2023-11-20 20:03:41,102 INFO     [HU] - Writing 35040 points in data/gen_HU_B19.csv...
2023-11-20 20:03:41,354 INFO     [HU] Gen Data process has finished successfully.
2023-11-20 20:03:41,354 INFO     [IT] Gen Data process has finished successfully.
2023-11-20 20:03:41,354 INFO     [PO] Gen Data process has finished successfully.
2023-11-20 20:03:41,354 INFO     [SP] Gen Data process has finished successfully.
2023-11-20 20:03:41,354 INFO     [UK] Gen Data process has finished successfully.
2023-11-20 20:03:41,354 INFO     [DE] Gen Data process has finished successfully.
2023-11-20 20:03:41,354 INFO     [DK] Gen Data process has finished successfully.
2023-11-20 20:03:41,354 INFO     [SE] Gen Data process has finished successfully.
2023-11-20 20:03:41,354 INFO     [NE] Gen Data process has finished successfully.
2023-11-20 20:03:41,354 INFO     <<<<<<<<<< Gem Data got successfully for all countries. >>>>>>>>>>
2023-11-20 20:03:41,354 INFO     Duration to get the gen data from entsoe: 246.16 seconds
2023-11-20 20:03:41,362 INFO     Starting raw_data process...
2023-11-20 20:04:03,875 INFO     Ensuring that all UnitName = 'MAW'...
2023-11-20 20:04:03,965 INFO     Ensuring that all data is between both dates
2023-11-20 20:04:04,048 INFO     Writing 298094 records C:\Users\cpazguzman\Documents\Repositories\Personal\EcoForecast-23\src\../data/raw_data.csv
2023-11-20 20:04:08,533 INFO     Raw data successfully generated.
2023-11-20 20:04:08,692 INFO     Duration to build the raw_data.csv by transforming the data collected: 27.33 seconds
2023-11-20 20:04:08,692 INFO     Data Ingestion total duration: 304.14 seconds
Mon Nov 20 20:04:09 HGMT 2023
Starting data processing...
2023-11-20 20:04:11,779 INFO     Starting...
2023-11-20 20:04:11,779 INFO     [Load data] Reading file data/raw_data.csv...
2023-11-20 20:04:12,568 INFO     [Load data] File read successfully
2023-11-20 20:04:12,568 INFO     [Clean data] Starting the process...
2023-11-20 20:04:12,568 INFO     [Clean data] Nº records before the cleaning: 298094
2023-11-20 20:04:12,844 INFO     [Clean data] Filtering AreaID != null
2023-11-20 20:04:12,911 INFO     [Clean data] After filtering AreaID != null we lose 120459 records
2023-11-20 20:04:12,920 INFO     [Clean data] Filtering by columns which are renewable
2023-11-20 20:04:12,936 INFO     [Clean data] Removing outliers...
2023-11-20 20:04:13,002 INFO     [Clean data] Outliers founds for DE = (B10=22,B12=25)
2023-11-20 20:04:13,018 INFO     [Clean data] No outliers found for DK
2023-11-20 20:04:13,060 INFO     [Clean data] Outliers founds for HU = (B15=3)
2023-11-20 20:04:13,069 INFO     [Clean data] Outliers founds for IT = (B09=4,B12=2)
2023-11-20 20:04:13,270 INFO     [Clean data] Process completed.
2023-11-20 20:04:13,270 INFO     [Preprocess data] Starting the process...
2023-11-20 20:04:13,270 INFO     [Preprocess data] Nº records before processing: 177635
2023-11-20 20:04:13,270 INFO     [Clean data] Treating nan values by interpolation by interval
2023-11-20 20:04:47,879 INFO     Interpolate interval finished for DE
2023-11-20 20:05:20,042 INFO     Interpolate interval finished for DK
2023-11-20 20:05:52,569 INFO     Interpolate interval finished for HU
2023-11-20 20:09:48,619 INFO     Interpolate interval finished for IT
2023-11-20 20:10:22,937 INFO     Interpolate interval finished for NE
2023-11-20 20:10:57,403 INFO     Interpolate interval finished for PO
2023-11-20 20:11:30,123 INFO     Interpolate interval finished for SE
2023-11-20 20:12:03,792 INFO     Interpolate interval finished for SP
2023-11-20 20:12:37,492 INFO     Interpolate interval finished for UK
2023-11-20 20:12:37,582 INFO     [Preprocess data] Executing resampling by hour
2023-11-20 20:12:37,729 INFO     [Preprocess data] Nº records after resampling: 78840
2023-11-20 20:12:37,729 INFO     [Preprocess data] Creating the columns green_energy and surplus...
2023-11-20 20:12:37,754 INFO     [Preprocess data] Creating the target table (StartTime, target)
2023-11-20 20:12:38,728 INFO     [Preprocess data] Pivoting the final table...
2023-11-20 20:12:38,824 INFO     [Preprocess data] Merging pivot table with target table
2023-11-20 20:12:38,848 INFO     [Preprocess data] Values with all rows without a null value: 1571
2023-11-20 20:12:38,848 INFO     [Preprocess data] Nº records after the processing: 8759 (Total lost = 168876)
2023-11-20 20:12:38,858 INFO     [Preprocess data] Value counts target: DK    6495
SE    1469
UK     754
DE      21
SP      20
Name: target, dtype: int64
2023-11-20 20:12:38,858 INFO     [Preprocess data] Process completed.
2023-11-20 20:12:38,858 INFO     [Save data] Saving the processing result in C:\Users\cpazguzman\Documents\Repositories\Personal\EcoForecast-23\src\../data/processed_data.csv
2023-11-20 20:12:39,846 INFO     [Save data] Process completed.
Starting model training...
2023-11-20 20:12:43,987 INFO     Starting...
2023-11-20 20:12:43,987 INFO     [Load Data] Loading training data...
2023-11-20 20:12:44,140 INFO     [Load Data] Training data successfully read.
2023-11-20 20:12:44,140 INFO     [Split Data] Spliting 80% train and 20% test...
2023-11-20 20:12:44,160 INFO     [Split Data] Size train = 7007; Size test = 1752
2023-11-20 20:12:44,160 INFO     [Save data] Saving train data in C:\Users\cpazguzman\Documents\Repositories\Personal\EcoForecast-23\src\../data/train_data.csv
2023-11-20 20:12:44,779 INFO     [Save data] Saving train data in C:\Users\cpazguzman\Documents\Repositories\Personal\EcoForecast-23\src\../data/test_data.csv
2023-11-20 20:12:44,965 INFO     [Train Model] Starting the training. After the training the model is automatically saved.
Warning: path already exists! This predictor may overwrite an existing predictor! path="C:\Users\cpazguzman\Documents\Repositories\Personal\EcoForecast-23\src\../models/model.pkl"
Presets specified: ['optimize_for_deployment']
Beginning AutoGluon training ...
AutoGluon will save models to "C:\Users\cpazguzman\Documents\Repositories\Personal\EcoForecast-23\src\../models/model.pkl\"
AutoGluon Version:  0.8.2
Python Version:     3.10.10
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.22621
Disk Space Avail:   162.89 GB / 255.28 GB (63.8%)
Train Data Rows:    7007
Train Data Columns: 118
Label Column: target
Preprocessing data ...
AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).
        5 unique label values:  ['DK', 'SE', 'UK', 'DE', 'SP']
        If 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])
Train Data Class Count: 5
Using Feature Generators to preprocess the data ...
Fitting AutoMLPipelineFeatureGenerator...
        Available Memory:                    3144.93 MB
        Train Data (Original)  Memory Usage: 7.09 MB (0.2% of available memory)
        Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.     
        Stage 1 Generators:
                Fitting AsTypeFeatureGenerator...
                        Note: Converting 4 features to boolean dtype as they only contain 2 unique values.
        Stage 2 Generators:
                Fitting FillNaFeatureGenerator...
        Stage 3 Generators:
                Fitting IdentityFeatureGenerator...
                Fitting DatetimeFeatureGenerator...
        Stage 4 Generators:
                Fitting DropUniqueFeatureGenerator...
        Stage 5 Generators:
                Fitting DropDuplicatesFeatureGenerator...
        Useless Original Features (Count: 37): ['B01_SE', 'B01_UK', 'B09_DK', 'B09_NE', 'B09_PO', 'B09_SE', 'B09_UK', 'B10_DK', 'B10_HU', 'B10_NE', 
'B10_SE', 'B10_UK', 'B11_DK', 'B11_NE', 'B11_SE', 'B11_UK', 'B12_DK', 'B12_NE', 'B12_UK', 'B13_DE', 'B13_DK', 'B13_HU', 'B13_IT', 'B13_NE', 'B13_PO', 'B13_UK', 'B15_DK', 'B15_IT', 'B15_NE', 'B15_PO', 'B15_SE', 'B15_UK', 'B16_UK', 'B18_HU', 'B18_PO', 'B18_SE', 'B18_UK']
                These features carry no predictive signal and should be manually investigated.
                This is typically a feature which has the same value for all rows.
                These features do not need to be present at inference time.
        Unused Original Features (Count: 3): ['B13_SP', 'B18_SP', 'green_energy_UK']
                These features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.
                Features can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.
                These features do not need to be present at inference time.
                ('float', []) : 3 | ['B13_SP', 'B18_SP', 'green_energy_UK']
        Types of features in original data (raw dtype, special dtypes):
                ('float', [])                      : 77 | ['B01_DE', 'B01_DK', 'B01_HU', 'B01_IT', 'B01_NE', ...]
                ('object', ['datetime_as_object']) :  1 | ['StartTime']
        Types of features in processed data (raw dtype, special dtypes):
                ('float', [])                : 75 | ['B01_DE', 'B01_DK', 'B01_HU', 'B01_IT', 'B01_NE', ...]
                ('int', ['bool'])            :  2 | ['B09_SP', 'B13_SE']
                ('int', ['datetime_as_int']) :  4 | ['StartTime', 'StartTime.month', 'StartTime.day', 'StartTime.dayofweek']
        0.8s = Fit runtime
        78 features in original data used to generate 81 features in processed data.
        Train Data (Processed) Memory Usage: 4.44 MB (0.1% of available memory)
Data preprocessing and feature engineering runtime = 0.85s ...
AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'
        To change this, specify the eval_metric parameter of Predictor()
Automatically generating train/validation split with holdout_frac=0.3, Train Rows: 4904, Val Rows: 2103
User-specified model hyperparameters to be fit:
{
        'NN_TORCH': {},
        'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],
        'CAT': {},
        'XGB': {},
        'FASTAI': {},
        'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_ar
gs': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],
        'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_ar
gs': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],
        'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],
}
Fitting 13 L1 models ...
Fitting model: KNeighborsUnif ...
        0.6155   = Validation score   (f1_macro)
        0.03s    = Training   runtime
        0.2s     = Validation runtime
Fitting model: KNeighborsDist ...
        0.6666   = Validation score   (f1_macro)
        0.06s    = Training   runtime
        0.07s    = Validation runtime
Fitting model: NeuralNetFastAI ...
        Warning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.
                Import fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==0.8.2`.
Fitting model: LightGBMXT ...
        Warning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.
                `import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.8.2`.
Fitting model: LightGBM ...
        Warning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.
                `import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.8.2`.
Fitting model: RandomForestGini ...
        0.6044   = Validation score   (f1_macro)
        4.18s    = Training   runtime
        0.16s    = Validation runtime
Fitting model: RandomForestEntr ...
        0.6052   = Validation score   (f1_macro)
        4.91s    = Training   runtime
        0.15s    = Validation runtime
Fitting model: CatBoost ...
        Warning: Exception caused CatBoost to fail during training (ImportError)... Skipping this model.
                `import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==0.8.2`.
Fitting model: ExtraTreesGini ...
        0.6336   = Validation score   (f1_macro)
        1.6s     = Training   runtime
        0.16s    = Validation runtime
Fitting model: ExtraTreesEntr ...
        0.6804   = Validation score   (f1_macro)
        1.73s    = Training   runtime
        0.19s    = Validation runtime
Fitting model: XGBoost ...
        Warning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.
                `import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==0.8.2`.
Fitting model: NeuralNetTorch ...
        Warning: Exception caused NeuralNetTorch to fail during training (ImportError)... Skipping this model.
                Unable to import dependency torch
A quick tip is to install via `pip install torch`.
The minimum torch version is currently 1.6.
Fitting model: LightGBMLarge ...
        Warning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.
                `import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==0.8.2`.
Fitting model: WeightedEnsemble_L2 ...
        0.6837   = Validation score   (f1_macro)
        1.66s    = Training   runtime
        0.0s     = Validation runtime
AutoGluon training complete, total runtime = 16.89s ... Best model: "WeightedEnsemble_L2"
Deleting model KNeighborsUnif. All files under models\model.pkl\models\KNeighborsUnif\ will be removed.
Deleting model KNeighborsDist. All files under models\model.pkl\models\KNeighborsDist\ will be removed.
Deleting model RandomForestEntr. All files under models\model.pkl\models\RandomForestEntr\ will be removed.
2023-11-20 20:13:06,864 INFO     Starting modeling predictor...
2023-11-20 20:13:06,872 INFO     [Load Data] Loading test data...
2023-11-20 20:13:06,921 INFO     [Load Data] Test data successfully read.
2023-11-20 20:13:06,921 INFO     [Load Data] Loading the model...
2023-11-20 20:13:07,052 INFO     [Load Data] Model successfully got.
2023-11-20 20:13:07,843 INFO     [Make Predictions] Evaluating model over test: {'f1_macro': 0.7751518086158283, 'accuracy': 0.9342105263157895, 'balanced_accuracy': 0.7121892603849748, 'mcc': 0.8366074482012534}
2023-11-20 20:13:07,843 INFO     [Make Predictions] Executing predictions...
2023-11-20 20:13:08,525 INFO     [Save Predictions] Writing the predictions in C:\Users\cpazguzman\Documents\Repositories\Personal\EcoForecast-23\src\../predictions/predictions.json
2023-11-20 20:13:08,535 INFO     [Save Predictions] Predictions successfully saved.
Pipeline completed.
```
***
# RESULTS: {'f1_macro': 0.7751518086158283, 'accuracy': 0.9342105263157895, 'balanced_accuracy': 0.7121892603849748, 'mcc': 0.8366074482012534}
***
REALIZADO POR Rarely Functional But Always F:
- RAMON SIEIRA MARTINEZ: www.linkedin.com/in/rasieira
- CESAR PAZ GUZMAN: www.linkedin.com/in/cesarpazguzman
